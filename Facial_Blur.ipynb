{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial_Blur.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinmsnow/ModSimPy/blob/master/Facial_Blur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qag1NL_FWy88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hey! this is wacky!!\n",
        "# can you see this?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t20OdGNZI7l",
        "colab_type": "text"
      },
      "source": [
        "yooo can you see me writing this?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4x1Y1xtZMAi",
        "colab_type": "code",
        "outputId": "cb1ed534-6b60-4b8d-c7cc-81a3e1b1ee8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!pip install torchviz\n",
        "# !CUDA_LAUNCH_BLOCKING=1\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy', 'eiffeltower.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy', 'bear.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy', 'airplane.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy', 'broccoli.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy', 'dog.npy', False)\n",
        "gdown.download('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy', 'broom.npy', False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.2.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy\n",
            "To: /content/eiffeltower.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 257MB/s] \n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "To: /content/bear.npy\n",
            "100%|██████████| 106M/106M [00:00<00:00, 256MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "To: /content/airplane.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 280MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy\n",
            "To: /content/broccoli.npy\n",
            "100%|██████████| 104M/104M [00:00<00:00, 201MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "To: /content/dog.npy\n",
            "100%|██████████| 119M/119M [00:00<00:00, 211MB/s]\n",
            "Downloading...\n",
            "From: https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
            "To: /content/broom.npy\n",
            "100%|██████████| 91.7M/91.7M [00:00<00:00, 240MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'broom.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPERGQfVgc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tower = np.load('eiffeltower.npy') #type = 1\n",
        "bear = np.load('bear.npy') # type = 0\n",
        "airplane = np.load('airplane.npy')\n",
        "broccoli = np.load('broccoli.npy')\n",
        "dog = np.load('dog.npy')\n",
        "broom = np.load('broom.npy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICO8bUo6V2He",
        "colab_type": "code",
        "outputId": "1003af2e-822e-462a-d447-13dda347097d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = airplane[750]\n",
        "X = np.resize(X,(28,28))\n",
        "X = np.invert(X)\n",
        "plt.imshow(X, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnoRPCJ8wWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yhVEn8Ql_h",
        "colab_type": "code",
        "outputId": "dcb847ad-f352-4849-d482-98315570932d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torch\n",
        "\n",
        "# class QuickDrawData(Dataset):\n",
        "#     def __init__(self, tower, bear, airplane, broccoli):\n",
        "#         super(QuickDrawData, self).__init__()\n",
        "#         self.data = np.vstack((tower, bear, airplane, broccoli))\n",
        "#         self.targets = np.concatenate((0*np.ones(tower.shape[0]), 1*np.ones(bear.shape[0]), 2*np.ones(airplane.shape[0]), 3*np.ones(broccoli.shape[0])))\n",
        "#         print(len(self.data))\n",
        "#         self.classes = ['tower', 'bear', 'airplane', 'broccoli']\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return self.targets.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, index):\n",
        "#         return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "class QuickDrawData(Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super(QuickDrawData, self).__init__()\n",
        "        count = 0\n",
        "        # self.data = np.empty(args[0].shape, dtype=int)\n",
        "        # self.targets = np.empty(args[0].shape, dtype=int)\n",
        "        self.classes = []\n",
        "        for arg in args:\n",
        "          # print(str(arg))\n",
        "          if type(arg) == str:\n",
        "            self.classes += arg\n",
        "          else:\n",
        "\n",
        "            if count == 0:\n",
        "              print(arg.shape)\n",
        "              self.data = np.array(arg)\n",
        "              self.targets = np.array(0*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              print(type(self.targets))\n",
        "              print(type(self.data))\n",
        "              print(type(self.classes))\n",
        "            else:\n",
        "              self.data = np.vstack((self.data, arg))\n",
        "              print(int(count)*np.ones(arg.shape[0], dtype = int))\n",
        "              print(self.targets)\n",
        "              self.targets = np.hstack((self.targets, int(count)*np.ones(arg.shape[0], dtype = int)))\n",
        "            count+=1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "quick_draw_data = QuickDrawData(tower, bear, airplane, broccoli, dog, broom, 'tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "im, target = quick_draw_data[55102]\n",
        "plt.imshow(im.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "im.shape\n",
        "print(target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134801, 784)\n",
            "[0 0 0 ... 0 0 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "[1 1 1 ... 1 1 1]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[2 2 2 ... 2 2 2]\n",
            "[0 0 0 ... 1 1 1]\n",
            "[3 3 3 ... 3 3 3]\n",
            "[0 0 0 ... 2 2 2]\n",
            "[4 4 4 ... 4 4 4]\n",
            "[0 0 0 ... 3 3 3]\n",
            "[5 5 5 ... 5 5 5]\n",
            "[0 0 0 ... 4 4 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICjCDwRF+IGgCD8QFOEHgiL8QFD/B8Pzhyim7SBFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi8X_pFZAcm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fMDbMQBAdAc",
        "colab": {}
      },
      "source": [
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import random_split\n",
        "\n",
        "# import torch\n",
        "\n",
        "# class QuickDrawData(Dataset):\n",
        "#     def __init__(self,airplane, broccoli):\n",
        "#         super(QuickDrawData, self).__init__()\n",
        "#         self.data = np.vstack((airplane, broccoli))\n",
        "#         self.targets = np.concatenate((0*np.ones(airplane.shape[0]), 1*np.ones(broccoli.shape[0])))\n",
        "#         print(len(self.data))\n",
        "#         # self.classes = ['tower', 'bear', 'airplane', 'broccoli']\n",
        "#         self.classes = ['airplane', 'broccoli']\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return self.targets.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, index):\n",
        "#         return torch.FloatTensor(self.data[index, :].reshape((28, 28))).unsqueeze(0), int(self.targets[index])\n",
        "\n",
        "# quick_draw_data = QuickDrawData(airplane, broccoli)\n",
        "\n",
        "# im, target = quick_draw_data[502]\n",
        "# plt.imshow(im.squeeze(), cmap='gray')\n",
        "# plt.show()\n",
        "# im.shape\n",
        "# print(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaebNQ7RwG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = int(len(quick_draw_data)*.9)\n",
        "train, test = random_split(quick_draw_data, [x,(len(quick_draw_data) - x)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMrsY0PERwJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data set information\n",
        "\n",
        "image_dims = 1, 28, 28\n",
        "n_training_samples = len(train) # How many training images to use\n",
        "n_test_samples = len(test) # How many test images to use\n",
        "classes = ('tower', 'bear', 'airplane', 'brocolli', 'dog', 'broom')\n",
        "\n",
        "# Load the training set\n",
        "train_set = train\n",
        "train_sampler = SubsetRandomSampler(\n",
        "    np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "test_set = test\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dv6-ImXRwMA",
        "colab_type": "code",
        "outputId": "b49fd80b-9f20-49eb-e9fd-6046737963f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test_set)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoJj27WRwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyCNN, self).__init__()\n",
        "    \n",
        "    num_kernels = 16\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.conv2 = nn.Conv2d(num_kernels, num_kernels*2, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "    self.maxpool_output_size1 = int(num_kernels*(image_dims[1]/2) * (image_dims[2]/2))\n",
        "    self.maxpool_output_size2 = int(num_kernels*(image_dims[1]/4) * (image_dims[2]/4)*2)\n",
        "\n",
        "    self.batchnorm1 = nn.BatchNorm2d(16)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "    \n",
        "    \n",
        "    fcl_size = 64\n",
        "    fcl_size2 = 32\n",
        "    self.fc1 = nn.Linear(1568, fcl_size)\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "    # fc2_size = fcl_size\n",
        "\n",
        "    self.fc2 = nn.Linear(fcl_size, fcl_size2)\n",
        "    # self.fc7 = nn.Linear(fcl_size, fcl_size)\n",
        "\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "    fc3_size = len(classes)\n",
        "    # fc3_size = 6\n",
        "    self.fc3 = nn.Linear(fcl_size2, fc3_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.batchnorm1(x)\n",
        "\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.batchnorm2(x)\n",
        "\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    # x = self.pool1(x)\n",
        "    # x = self.activation_func(x)\n",
        "    # x = x.view(-1, self.maxpool_output_size1)\n",
        "    # x = self.pool2(x)\n",
        "    # x = self.activation_func(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = x.view(-1, self.maxpool_output_size2)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    x = self.activation_func(x)\n",
        "    # x = self.fc2(x)\n",
        "    # x = self.activation_func(x)\n",
        "    x = self.fc3(x)\n",
        "    x = torch.nn.functional.log_softmax(x)\n",
        "    # x = self.activation_func(x)\n",
        "\n",
        "    # x = self.activation_func7(x)\n",
        "    return x\n",
        "\n",
        "  def get_loss(self, learning_rate):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    return loss, optimizer\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHaayoLKRwaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MyCNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SbsGk9qjpmY",
        "colab_type": "code",
        "outputId": "aa4963a6-9d37-47a6-8c47-963973cc291c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1, image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# \n",
        "visualize_network(net)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f91501e8278>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"524pt\" height=\"864pt\"\n viewBox=\"0.00 0.00 523.56 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.8067 .8067) rotate(0) translate(4 1067)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1067 645,-1067 645,4 -4,4\"/>\n<!-- 140262091163520 -->\n<g id=\"node1\" class=\"node\">\n<title>140262091163520</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"537,-21 412,-21 412,0 537,0 537,-21\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n</g>\n<!-- 140262091164248 -->\n<g id=\"node2\" class=\"node\">\n<title>140262091164248</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"526.5,-78 422.5,-78 422.5,-57 526.5,-57 526.5,-78\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140262091164248&#45;&gt;140262091163520 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140262091164248&#45;&gt;140262091163520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.5,-56.7787C474.5,-49.6134 474.5,-39.9517 474.5,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.0001,-31.1732 474.5,-21.1732 471.0001,-31.1732 478.0001,-31.1732\"/>\n</g>\n<!-- 140262090834104 -->\n<g id=\"node3\" class=\"node\">\n<title>140262090834104</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"409.5,-148 355.5,-148 355.5,-114 409.5,-114 409.5,-148\"/>\n<text text-anchor=\"middle\" x=\"382.5\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.bias</text>\n<text text-anchor=\"middle\" x=\"382.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6)</text>\n</g>\n<!-- 140262090834104&#45;&gt;140262091164248 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140262090834104&#45;&gt;140262091164248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M407.1543,-113.9832C420.6894,-104.641 437.3926,-93.1122 450.778,-83.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"452.7865,-86.7398 459.0283,-78.1788 448.8102,-80.9788 452.7865,-86.7398\"/>\n</g>\n<!-- 140262090834496 -->\n<g id=\"node4\" class=\"node\">\n<title>140262090834496</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"521.5,-141.5 427.5,-141.5 427.5,-120.5 521.5,-120.5 521.5,-141.5\"/>\n<text text-anchor=\"middle\" x=\"474.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140262090834496&#45;&gt;140262091164248 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140262090834496&#45;&gt;140262091164248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M474.5,-120.2281C474.5,-111.5091 474.5,-98.9699 474.5,-88.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"478.0001,-88.1128 474.5,-78.1128 471.0001,-88.1129 478.0001,-88.1128\"/>\n</g>\n<!-- 140262090835168 -->\n<g id=\"node5\" class=\"node\">\n<title>140262090835168</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"525.5,-211.5 421.5,-211.5 421.5,-190.5 525.5,-190.5 525.5,-211.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140262090835168&#45;&gt;140262090834496 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140262090835168&#45;&gt;140262090834496</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.6519,-190.3685C473.7972,-180.1925 474.0206,-164.5606 474.2016,-151.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.7034,-151.7806 474.3467,-141.7315 470.7041,-151.6805 477.7034,-151.7806\"/>\n</g>\n<!-- 140262090835392 -->\n<g id=\"node6\" class=\"node\">\n<title>140262090835392</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"408.5,-288 354.5,-288 354.5,-254 408.5,-254 408.5,-288\"/>\n<text text-anchor=\"middle\" x=\"381.5\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"381.5\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140262090835392&#45;&gt;140262090835168 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140262090835392&#45;&gt;140262090835168</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M404.2416,-253.6966C418.6068,-242.7666 437.0787,-228.7119 451.3325,-217.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"453.7491,-220.4258 459.5881,-211.5852 449.5104,-214.855 453.7491,-220.4258\"/>\n</g>\n<!-- 140262090835224 -->\n<g id=\"node7\" class=\"node\">\n<title>140262090835224</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520.5,-281.5 426.5,-281.5 426.5,-260.5 520.5,-260.5 520.5,-281.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140262090835224&#45;&gt;140262090835168 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140262090835224&#45;&gt;140262090835168</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-260.3685C473.5,-250.1925 473.5,-234.5606 473.5,-221.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-221.7315 473.5,-211.7315 470.0001,-221.7316 477.0001,-221.7315\"/>\n</g>\n<!-- 140262090835112 -->\n<g id=\"node8\" class=\"node\">\n<title>140262090835112</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"525.5,-351.5 421.5,-351.5 421.5,-330.5 525.5,-330.5 525.5,-351.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-337.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140262090835112&#45;&gt;140262090835224 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140262090835112&#45;&gt;140262090835224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-330.3685C473.5,-320.1925 473.5,-304.5606 473.5,-291.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-291.7315 473.5,-281.7315 470.0001,-291.7316 477.0001,-291.7315\"/>\n</g>\n<!-- 140262090835560 -->\n<g id=\"node9\" class=\"node\">\n<title>140262090835560</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"409.5,-428 355.5,-428 355.5,-394 409.5,-394 409.5,-428\"/>\n<text text-anchor=\"middle\" x=\"382.5\" y=\"-414.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"382.5\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 140262090835560&#45;&gt;140262090835112 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140262090835560&#45;&gt;140262090835112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M404.9944,-393.6966C419.2034,-382.7666 437.4745,-368.7119 451.5735,-357.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"453.947,-360.4565 459.7393,-351.5852 449.679,-354.9081 453.947,-360.4565\"/>\n</g>\n<!-- 140262090836512 -->\n<g id=\"node10\" class=\"node\">\n<title>140262090836512</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"519,-421.5 428,-421.5 428,-400.5 519,-400.5 519,-421.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 140262090836512&#45;&gt;140262090835112 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140262090836512&#45;&gt;140262090835112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-400.3685C473.5,-390.1925 473.5,-374.5606 473.5,-361.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-361.7315 473.5,-351.7315 470.0001,-361.7316 477.0001,-361.7315\"/>\n</g>\n<!-- 140262090994520 -->\n<g id=\"node11\" class=\"node\">\n<title>140262090994520</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"520.5,-491.5 426.5,-491.5 426.5,-470.5 520.5,-470.5 520.5,-491.5\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-477.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140262090994520&#45;&gt;140262090836512 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140262090994520&#45;&gt;140262090836512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-470.3685C473.5,-460.1925 473.5,-444.5606 473.5,-431.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-431.7315 473.5,-421.7315 470.0001,-431.7316 477.0001,-431.7315\"/>\n</g>\n<!-- 140262090994240 -->\n<g id=\"node12\" class=\"node\">\n<title>140262090994240</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"563.5,-555 383.5,-555 383.5,-534 563.5,-534 563.5,-555\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-541.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140262090994240&#45;&gt;140262090994520 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140262090994240&#45;&gt;140262090994520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-533.7281C473.5,-525.0091 473.5,-512.4699 473.5,-501.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-501.6128 473.5,-491.6128 470.0001,-501.6129 477.0001,-501.6128\"/>\n</g>\n<!-- 140262090994632 -->\n<g id=\"node13\" class=\"node\">\n<title>140262090994632</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"550,-612 397,-612 397,-591 550,-591 550,-612\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-598.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140262090994632&#45;&gt;140262090994240 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140262090994632&#45;&gt;140262090994240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-590.7787C473.5,-583.6134 473.5,-573.9517 473.5,-565.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-565.1732 473.5,-555.1732 470.0001,-565.1732 477.0001,-565.1732\"/>\n</g>\n<!-- 140262090995864 -->\n<g id=\"node14\" class=\"node\">\n<title>140262090995864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"401.5,-675.5 307.5,-675.5 307.5,-654.5 401.5,-654.5 401.5,-675.5\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-661.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140262090995864&#45;&gt;140262090994632 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140262090995864&#45;&gt;140262090994632</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M374.4179,-654.3715C393.553,-644.1608 422.6839,-628.6162 444.4131,-617.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.3423,-619.9589 453.5171,-612.1631 443.0468,-613.7831 446.3423,-619.9589\"/>\n</g>\n<!-- 140262091102416 -->\n<g id=\"node15\" class=\"node\">\n<title>140262091102416</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"433,-739 276,-739 276,-718 433,-718 433,-739\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-725.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140262091102416&#45;&gt;140262090995864 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140262091102416&#45;&gt;140262090995864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-717.7281C354.5,-709.0091 354.5,-696.4699 354.5,-685.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-685.6128 354.5,-675.6128 351.0001,-685.6129 358.0001,-685.6128\"/>\n</g>\n<!-- 140262091101128 -->\n<g id=\"node16\" class=\"node\">\n<title>140262091101128</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"295.5,-802.5 115.5,-802.5 115.5,-781.5 295.5,-781.5 295.5,-802.5\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-788.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 140262091101128&#45;&gt;140262091102416 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140262091101128&#45;&gt;140262091102416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M230.4392,-781.3715C255.0429,-770.8861 292.8449,-754.7758 320.2558,-743.094\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"321.6522,-746.3036 329.4794,-739.1631 318.9077,-739.864 321.6522,-746.3036\"/>\n</g>\n<!-- 140262091102136 -->\n<g id=\"node17\" class=\"node\">\n<title>140262091102136</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"282,-866 129,-866 129,-845 282,-845 282,-866\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-852.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnBatchNormBackward</text>\n</g>\n<!-- 140262091102136&#45;&gt;140262091101128 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140262091102136&#45;&gt;140262091101128</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-844.7281C205.5,-836.0091 205.5,-823.4699 205.5,-812.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-812.6128 205.5,-802.6128 202.0001,-812.6129 209.0001,-812.6128\"/>\n</g>\n<!-- 140262091103704 -->\n<g id=\"node18\" class=\"node\">\n<title>140262091103704</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"133.5,-929.5 39.5,-929.5 39.5,-908.5 133.5,-908.5 133.5,-929.5\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-915.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140262091103704&#45;&gt;140262091102136 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140262091103704&#45;&gt;140262091102136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.4179,-908.3715C125.553,-898.1608 154.6839,-882.6162 176.4131,-871.0212\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.3423,-873.9589 185.5171,-866.1631 175.0468,-867.7831 178.3423,-873.9589\"/>\n</g>\n<!-- 140262091103984 -->\n<g id=\"node19\" class=\"node\">\n<title>140262091103984</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-993 8,-993 8,-972 165,-972 165,-993\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-979.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 140262091103984&#45;&gt;140262091103704 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140262091103984&#45;&gt;140262091103704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M86.5,-971.7281C86.5,-963.0091 86.5,-950.4699 86.5,-939.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"90.0001,-939.6128 86.5,-929.6128 83.0001,-939.6129 90.0001,-939.6128\"/>\n</g>\n<!-- 140262091103648 -->\n<g id=\"node20\" class=\"node\">\n<title>140262091103648</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"81,-1063 0,-1063 0,-1029 81,-1029 81,-1063\"/>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-1036.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 1, 3, 3)</text>\n</g>\n<!-- 140262091103648&#45;&gt;140262091103984 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140262091103648&#45;&gt;140262091103984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M52.8272,-1028.9832C58.9107,-1020.5853 66.2742,-1010.4204 72.5621,-1001.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5945,-1003.5204 78.6266,-993.3687 69.9256,-999.4138 75.5945,-1003.5204\"/>\n</g>\n<!-- 140262091102808 -->\n<g id=\"node21\" class=\"node\">\n<title>140262091102808</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"167.5,-1063 99.5,-1063 99.5,-1029 167.5,-1029 167.5,-1063\"/>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1049.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"133.5\" y=\"-1036.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140262091102808&#45;&gt;140262091103984 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140262091102808&#45;&gt;140262091103984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.9049,-1028.9832C114.6237,-1020.4969 107.0069,-1010.2062 100.5384,-1001.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3071,-999.3243 94.5445,-993.3687 97.6806,-1003.4888 103.3071,-999.3243\"/>\n</g>\n<!-- 140262091103032 -->\n<g id=\"node22\" class=\"node\">\n<title>140262091103032</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"259.5,-936 151.5,-936 151.5,-902 259.5,-902 259.5,-936\"/>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.weight</text>\n<text text-anchor=\"middle\" x=\"205.5\" y=\"-909.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140262091103032&#45;&gt;140262091102136 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140262091103032&#45;&gt;140262091102136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M205.5,-901.9832C205.5,-894.1157 205.5,-884.6973 205.5,-876.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0001,-876.3686 205.5,-866.3687 202.0001,-876.3687 209.0001,-876.3686\"/>\n</g>\n<!-- 140262091103760 -->\n<g id=\"node23\" class=\"node\">\n<title>140262091103760</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"373,-936 278,-936 278,-902 373,-902 373,-936\"/>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-922.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm1.bias</text>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-909.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 140262091103760&#45;&gt;140262091102136 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140262091103760&#45;&gt;140262091102136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293.3422,-901.9832C275.1833,-892.3741 252.6526,-880.4516 234.9561,-871.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.1563,-867.7625 225.6804,-866.1788 232.8822,-873.9496 236.1563,-867.7625\"/>\n</g>\n<!-- 140262091101800 -->\n<g id=\"node24\" class=\"node\">\n<title>140262091101800</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"395,-809 314,-809 314,-775 395,-775 395,-809\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-782.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 16, 3, 3)</text>\n</g>\n<!-- 140262091101800&#45;&gt;140262091102416 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140262091101800&#45;&gt;140262091102416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-774.9832C354.5,-767.1157 354.5,-757.6973 354.5,-749.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-749.3686 354.5,-739.3687 351.0001,-749.3687 358.0001,-749.3686\"/>\n</g>\n<!-- 140262091102192 -->\n<g id=\"node25\" class=\"node\">\n<title>140262091102192</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"481.5,-809 413.5,-809 413.5,-775 481.5,-775 481.5,-809\"/>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-795.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n<text text-anchor=\"middle\" x=\"447.5\" y=\"-782.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140262091102192&#45;&gt;140262091102416 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140262091102192&#45;&gt;140262091102416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M422.5777,-774.9832C408.8955,-765.641 392.0107,-754.1122 378.4799,-744.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.372,-741.9273 370.1398,-739.1788 376.4248,-747.7082 380.372,-741.9273\"/>\n</g>\n<!-- 140262090993736 -->\n<g id=\"node26\" class=\"node\">\n<title>140262090993736</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"527.5,-682 419.5,-682 419.5,-648 527.5,-648 527.5,-682\"/>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-668.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.weight</text>\n<text text-anchor=\"middle\" x=\"473.5\" y=\"-655.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140262090993736&#45;&gt;140262090994632 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140262090993736&#45;&gt;140262090994632</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M473.5,-647.9832C473.5,-640.1157 473.5,-630.6973 473.5,-622.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"477.0001,-622.3686 473.5,-612.3687 470.0001,-622.3687 477.0001,-622.3686\"/>\n</g>\n<!-- 140262090996368 -->\n<g id=\"node27\" class=\"node\">\n<title>140262090996368</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"641,-682 546,-682 546,-648 641,-648 641,-682\"/>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-668.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">batchnorm2.bias</text>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-655.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 140262090996368&#45;&gt;140262090994632 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140262090996368&#45;&gt;140262090994632</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M561.3422,-647.9832C543.1833,-638.3741 520.6526,-626.4516 502.9561,-617.0872\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"504.1563,-613.7625 493.6804,-612.1788 500.8822,-619.9496 504.1563,-613.7625\"/>\n</g>\n<!-- 140262090995472 -->\n<g id=\"node28\" class=\"node\">\n<title>140262090995472</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"610,-421.5 537,-421.5 537,-400.5 610,-400.5 610,-421.5\"/>\n<text text-anchor=\"middle\" x=\"573.5\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140262090995472&#45;&gt;140262090835112 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140262090995472&#45;&gt;140262090835112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M558.3122,-400.3685C541.992,-388.9444 515.8486,-370.644 496.898,-357.3786\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"498.7026,-354.3695 488.5031,-351.5022 494.6883,-360.1042 498.7026,-354.3695\"/>\n</g>\n<!-- 140262090995136 -->\n<g id=\"node29\" class=\"node\">\n<title>140262090995136</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"608,-498 539,-498 539,-464 608,-464 608,-498\"/>\n<text text-anchor=\"middle\" x=\"573.5\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"573.5\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 1568)</text>\n</g>\n<!-- 140262090995136&#45;&gt;140262090995472 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140262090995136&#45;&gt;140262090995472</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M573.5,-463.6966C573.5,-454.0634 573.5,-442.003 573.5,-431.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"577.0001,-431.7912 573.5,-421.7913 570.0001,-431.7913 577.0001,-431.7912\"/>\n</g>\n<!-- 140262090835280 -->\n<g id=\"node30\" class=\"node\">\n<title>140262090835280</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"613,-281.5 540,-281.5 540,-260.5 613,-260.5 613,-281.5\"/>\n<text text-anchor=\"middle\" x=\"576.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140262090835280&#45;&gt;140262090835168 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140262090835280&#45;&gt;140262090835168</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M560.8565,-260.3685C543.9707,-248.8927 516.8752,-230.4783 497.3354,-217.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"499.1913,-214.2284 488.9532,-211.5022 495.2567,-220.0179 499.1913,-214.2284\"/>\n</g>\n<!-- 140262090835448 -->\n<g id=\"node31\" class=\"node\">\n<title>140262090835448</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"611,-358 544,-358 544,-324 611,-324 611,-358\"/>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-331.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 64)</text>\n</g>\n<!-- 140262090835448&#45;&gt;140262090835280 -->\n<g id=\"edge30\" class=\"edge\">\n<title>140262090835448&#45;&gt;140262090835280</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M577.2528,-323.6966C577.1152,-314.0634 576.9429,-302.003 576.7979,-291.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"580.2967,-291.7402 576.6542,-281.7913 573.2975,-291.8403 580.2967,-291.7402\"/>\n</g>\n<!-- 140262090834720 -->\n<g id=\"node32\" class=\"node\">\n<title>140262090834720</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"614,-141.5 541,-141.5 541,-120.5 614,-120.5 614,-141.5\"/>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140262090834720&#45;&gt;140262091164248 -->\n<g id=\"edge31\" class=\"edge\">\n<title>140262090834720&#45;&gt;140262091164248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M560.0275,-120.2281C543.6519,-110.1325 518.9682,-94.9149 500.3209,-83.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"502.0636,-80.3814 491.7145,-78.1128 498.3901,-86.3401 502.0636,-80.3814\"/>\n</g>\n<!-- 140262090835336 -->\n<g id=\"node33\" class=\"node\">\n<title>140262090835336</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"611,-218 544,-218 544,-184 611,-184 611,-218\"/>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.weight</text>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (6, 32)</text>\n</g>\n<!-- 140262090835336&#45;&gt;140262090834720 -->\n<g id=\"edge32\" class=\"edge\">\n<title>140262090835336&#45;&gt;140262090834720</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M577.5,-183.6966C577.5,-174.0634 577.5,-162.003 577.5,-151.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"581.0001,-151.7912 577.5,-141.7913 574.0001,-151.7913 581.0001,-151.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9CTlb4Yx7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 5\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, sampler=train_sampler, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, sampler=test_sampler, num_workers = 2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 200\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGyZdjgNjf_h",
        "colab_type": "code",
        "outputId": "4f5cf69d-385d-4984-9583-02fbdb2ab898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 200\t train_loss: 0.67 took: 1.79s\n",
            "Epoch 1, Iteration 400\t train_loss: 0.49 took: 1.65s\n",
            "Epoch 1, Iteration 600\t train_loss: 0.43 took: 1.71s\n",
            "Epoch 1, Iteration 800\t train_loss: 0.40 took: 1.64s\n",
            "Epoch 1, Iteration 1000\t train_loss: 0.37 took: 1.66s\n",
            "Epoch 1, Iteration 1200\t train_loss: 0.36 took: 1.57s\n",
            "Epoch 1, Iteration 1400\t train_loss: 0.37 took: 1.69s\n",
            "Epoch 1, Iteration 1600\t train_loss: 0.34 took: 1.64s\n",
            "Epoch 1, Iteration 1800\t train_loss: 0.34 took: 1.62s\n",
            "Epoch 1, Iteration 2000\t train_loss: 0.34 took: 1.60s\n",
            "Epoch 1, Iteration 2200\t train_loss: 0.35 took: 1.64s\n",
            "Epoch 1, Iteration 2400\t train_loss: 0.31 took: 1.74s\n",
            "Epoch 1, Iteration 2600\t train_loss: 0.34 took: 1.62s\n",
            "Epoch 1, Iteration 2800\t train_loss: 0.34 took: 1.61s\n",
            "Epoch 1, Iteration 3000\t train_loss: 0.35 took: 1.70s\n",
            "Epoch 1, Iteration 3200\t train_loss: 0.32 took: 1.62s\n",
            "Epoch 1, Iteration 3400\t train_loss: 0.32 took: 1.62s\n",
            "Epoch 1, Iteration 3600\t train_loss: 0.33 took: 1.62s\n",
            "Epoch 1, Iteration 3800\t train_loss: 0.31 took: 1.58s\n",
            "Epoch 1, Iteration 4000\t train_loss: 0.33 took: 1.69s\n",
            "Epoch 1, Iteration 4200\t train_loss: 0.32 took: 1.69s\n",
            "Epoch 1, Iteration 4400\t train_loss: 0.33 took: 1.68s\n",
            "Epoch 1, Iteration 4600\t train_loss: 0.32 took: 1.64s\n",
            "Epoch 1, Iteration 4800\t train_loss: 0.30 took: 1.63s\n",
            "Epoch 1, Iteration 5000\t train_loss: 0.30 took: 1.66s\n",
            "Epoch 1, Iteration 5200\t train_loss: 0.32 took: 1.69s\n",
            "Epoch 1, Iteration 5400\t train_loss: 0.32 took: 1.62s\n",
            "Epoch 1, Iteration 5600\t train_loss: 0.31 took: 1.63s\n",
            "Epoch 1, Iteration 5800\t train_loss: 0.32 took: 1.67s\n",
            "Epoch 1, Iteration 6000\t train_loss: 0.31 took: 1.66s\n",
            "Epoch 1, Iteration 6200\t train_loss: 0.30 took: 1.64s\n",
            "Epoch 1, Iteration 6400\t train_loss: 0.33 took: 1.74s\n",
            "Epoch 1, Iteration 6600\t train_loss: 0.30 took: 1.81s\n",
            "Epoch 1, Iteration 6800\t train_loss: 0.32 took: 1.72s\n",
            "Epoch 1, Iteration 7000\t train_loss: 0.29 took: 1.63s\n",
            "Epoch 1, Iteration 7200\t train_loss: 0.28 took: 1.69s\n",
            "Epoch 1, Iteration 7400\t train_loss: 0.30 took: 1.64s\n",
            "Epoch 1, Iteration 7600\t train_loss: 0.31 took: 1.64s\n",
            "Epoch 1, Iteration 7800\t train_loss: 0.29 took: 1.62s\n",
            "Epoch 1, Iteration 8000\t train_loss: 0.30 took: 1.69s\n",
            "Epoch 1, Iteration 8200\t train_loss: 0.30 took: 1.65s\n",
            "Epoch 1, Iteration 8400\t train_loss: 0.29 took: 1.67s\n",
            "Epoch 1, Iteration 8600\t train_loss: 0.28 took: 1.64s\n",
            "Epoch 1, Iteration 8800\t train_loss: 0.29 took: 1.66s\n",
            "Epoch 1, Iteration 9000\t train_loss: 0.31 took: 1.67s\n",
            "Epoch 1, Iteration 9200\t train_loss: 0.30 took: 1.71s\n",
            "Epoch 1, Iteration 9400\t train_loss: 0.30 took: 1.66s\n",
            "Epoch 1, Iteration 9600\t train_loss: 0.30 took: 1.64s\n",
            "Epoch 1, Iteration 9800\t train_loss: 0.29 took: 1.68s\n",
            "Epoch 1, Iteration 10000\t train_loss: 0.29 took: 1.65s\n",
            "Epoch 1, Iteration 10200\t train_loss: 0.28 took: 1.69s\n",
            "Epoch 1, Iteration 10400\t train_loss: 0.30 took: 1.70s\n",
            "Epoch 1, Iteration 10600\t train_loss: 0.28 took: 1.64s\n",
            "Epoch 1, Iteration 10800\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 1, Iteration 11000\t train_loss: 0.29 took: 1.72s\n",
            "Epoch 1, Iteration 11200\t train_loss: 0.30 took: 1.67s\n",
            "Epoch 1, Iteration 11400\t train_loss: 0.28 took: 1.59s\n",
            "Epoch 1, Iteration 11600\t train_loss: 0.31 took: 1.64s\n",
            "Epoch 1, Iteration 11800\t train_loss: 0.28 took: 1.62s\n",
            "Epoch 1, Iteration 12000\t train_loss: 0.28 took: 1.66s\n",
            "Epoch 1, Iteration 12200\t train_loss: 0.30 took: 1.66s\n",
            "Epoch 1, Iteration 12400\t train_loss: 0.30 took: 1.75s\n",
            "Epoch 1, Iteration 12600\t train_loss: 0.28 took: 1.61s\n",
            "Epoch 1, Iteration 12800\t train_loss: 0.32 took: 1.66s\n",
            "Epoch 1, Iteration 13000\t train_loss: 0.28 took: 1.61s\n",
            "Epoch 1, Iteration 13200\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 1, Iteration 13400\t train_loss: 0.31 took: 1.61s\n",
            "Epoch 1, Iteration 13600\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 1, Iteration 13800\t train_loss: 0.28 took: 1.65s\n",
            "Epoch 1, Iteration 14000\t train_loss: 0.30 took: 1.71s\n",
            "Epoch 1, Iteration 14200\t train_loss: 0.27 took: 1.59s\n",
            "Epoch 1, Iteration 14400\t train_loss: 0.28 took: 1.64s\n",
            "Epoch 1, Iteration 14600\t train_loss: 0.28 took: 1.76s\n",
            "Epoch 1, Iteration 14800\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 1, Iteration 15000\t train_loss: 0.28 took: 1.72s\n",
            "Epoch 1, Iteration 15200\t train_loss: 0.27 took: 1.64s\n",
            "Epoch 1, Iteration 15400\t train_loss: 0.28 took: 1.69s\n",
            "Epoch 1, Iteration 15600\t train_loss: 0.27 took: 1.62s\n",
            "Epoch 1, Iteration 15800\t train_loss: 0.29 took: 1.73s\n",
            "Epoch 1, Iteration 16000\t train_loss: 0.27 took: 1.69s\n",
            "Epoch 1, Iteration 16200\t train_loss: 0.28 took: 1.61s\n",
            "Epoch 1, Iteration 16400\t train_loss: 0.27 took: 1.68s\n",
            "Epoch 1, Iteration 16600\t train_loss: 0.28 took: 1.63s\n",
            "Epoch 1, Iteration 16800\t train_loss: 0.29 took: 1.66s\n",
            "Epoch 1, Iteration 17000\t train_loss: 0.28 took: 1.72s\n",
            "Epoch 1, Iteration 17200\t train_loss: 0.27 took: 1.70s\n",
            "Epoch 1, Iteration 17400\t train_loss: 0.27 took: 1.81s\n",
            "Epoch 1, Iteration 17600\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 1, Iteration 17800\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 1, Iteration 18000\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 1, Iteration 18200\t train_loss: 0.27 took: 1.65s\n",
            "Epoch 1, Iteration 18400\t train_loss: 0.28 took: 1.71s\n",
            "Epoch 1, Iteration 18600\t train_loss: 0.27 took: 1.60s\n",
            "Epoch 1, Iteration 18800\t train_loss: 0.28 took: 1.64s\n",
            "Epoch 1, Iteration 19000\t train_loss: 0.28 took: 1.68s\n",
            "Epoch 1, Iteration 19200\t train_loss: 0.28 took: 1.60s\n",
            "Epoch 1, Iteration 19400\t train_loss: 0.26 took: 1.61s\n",
            "Epoch 1, Iteration 19600\t train_loss: 0.27 took: 1.68s\n",
            "Epoch 1, Iteration 19800\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 1, Iteration 20000\t train_loss: 0.29 took: 1.63s\n",
            "Epoch 1, Iteration 20200\t train_loss: 0.27 took: 1.62s\n",
            "Epoch 1, Iteration 20400\t train_loss: 0.27 took: 1.62s\n",
            "Epoch 1, Iteration 20600\t train_loss: 0.27 took: 1.62s\n",
            "Epoch 1, Iteration 20800\t train_loss: 0.28 took: 1.60s\n",
            "Epoch 1, Iteration 21000\t train_loss: 0.27 took: 1.68s\n",
            "Epoch 1, Iteration 21200\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 1, Iteration 21400\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 1, Iteration 21600\t train_loss: 0.28 took: 1.67s\n",
            "Epoch 1, Iteration 21800\t train_loss: 0.27 took: 1.71s\n",
            "Epoch 1, Iteration 22000\t train_loss: 0.28 took: 1.62s\n",
            "Epoch 1, Iteration 22200\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 1, Iteration 22400\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 1, Iteration 22600\t train_loss: 0.29 took: 1.65s\n",
            "Epoch 1, Iteration 22800\t train_loss: 0.27 took: 1.62s\n",
            "Epoch 1, Iteration 23000\t train_loss: 0.29 took: 1.62s\n",
            "Validation loss = 0.26\n",
            "Epoch 2, Iteration 200\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 2, Iteration 400\t train_loss: 0.25 took: 1.70s\n",
            "Epoch 2, Iteration 600\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 2, Iteration 800\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 2, Iteration 1000\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 2, Iteration 1200\t train_loss: 0.27 took: 1.67s\n",
            "Epoch 2, Iteration 1400\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 2, Iteration 1600\t train_loss: 0.27 took: 1.65s\n",
            "Epoch 2, Iteration 1800\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 2, Iteration 2000\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 2, Iteration 2200\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 2, Iteration 2400\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 2, Iteration 2600\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 2800\t train_loss: 0.27 took: 1.67s\n",
            "Epoch 2, Iteration 3000\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 2, Iteration 3200\t train_loss: 0.26 took: 1.84s\n",
            "Epoch 2, Iteration 3400\t train_loss: 0.27 took: 1.85s\n",
            "Epoch 2, Iteration 3600\t train_loss: 0.26 took: 1.85s\n",
            "Epoch 2, Iteration 3800\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 2, Iteration 4000\t train_loss: 0.27 took: 1.67s\n",
            "Epoch 2, Iteration 4200\t train_loss: 0.27 took: 1.69s\n",
            "Epoch 2, Iteration 4400\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 2, Iteration 4600\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 2, Iteration 4800\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 5000\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 2, Iteration 5200\t train_loss: 0.28 took: 1.69s\n",
            "Epoch 2, Iteration 5400\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 2, Iteration 5600\t train_loss: 0.27 took: 1.66s\n",
            "Epoch 2, Iteration 5800\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 2, Iteration 6000\t train_loss: 0.27 took: 1.65s\n",
            "Epoch 2, Iteration 6200\t train_loss: 0.26 took: 1.81s\n",
            "Epoch 2, Iteration 6400\t train_loss: 0.27 took: 1.62s\n",
            "Epoch 2, Iteration 6600\t train_loss: 0.28 took: 1.65s\n",
            "Epoch 2, Iteration 6800\t train_loss: 0.28 took: 1.63s\n",
            "Epoch 2, Iteration 7000\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 2, Iteration 7200\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 2, Iteration 7400\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 7600\t train_loss: 0.27 took: 1.69s\n",
            "Epoch 2, Iteration 7800\t train_loss: 0.27 took: 1.63s\n",
            "Epoch 2, Iteration 8000\t train_loss: 0.28 took: 1.62s\n",
            "Epoch 2, Iteration 8200\t train_loss: 0.26 took: 1.61s\n",
            "Epoch 2, Iteration 8400\t train_loss: 0.28 took: 1.69s\n",
            "Epoch 2, Iteration 8600\t train_loss: 0.29 took: 1.66s\n",
            "Epoch 2, Iteration 8800\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 2, Iteration 9000\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 2, Iteration 9200\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 2, Iteration 9400\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 2, Iteration 9600\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 2, Iteration 9800\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 2, Iteration 10000\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 2, Iteration 10200\t train_loss: 0.26 took: 1.69s\n",
            "Epoch 2, Iteration 10400\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 2, Iteration 10600\t train_loss: 0.26 took: 1.59s\n",
            "Epoch 2, Iteration 10800\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 2, Iteration 11000\t train_loss: 0.27 took: 1.65s\n",
            "Epoch 2, Iteration 11200\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 2, Iteration 11400\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 2, Iteration 11600\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 2, Iteration 11800\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 12000\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 12200\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 12400\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 12600\t train_loss: 0.26 took: 1.71s\n",
            "Epoch 2, Iteration 12800\t train_loss: 0.27 took: 1.68s\n",
            "Epoch 2, Iteration 13000\t train_loss: 0.26 took: 1.69s\n",
            "Epoch 2, Iteration 13200\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 2, Iteration 13400\t train_loss: 0.25 took: 1.73s\n",
            "Epoch 2, Iteration 13600\t train_loss: 0.28 took: 1.64s\n",
            "Epoch 2, Iteration 13800\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 2, Iteration 14000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 2, Iteration 14200\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 2, Iteration 14400\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 2, Iteration 14600\t train_loss: 0.27 took: 1.65s\n",
            "Epoch 2, Iteration 14800\t train_loss: 0.26 took: 1.77s\n",
            "Epoch 2, Iteration 15000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 2, Iteration 15200\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 2, Iteration 15400\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 2, Iteration 15600\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 2, Iteration 15800\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 2, Iteration 16000\t train_loss: 0.26 took: 1.81s\n",
            "Epoch 2, Iteration 16200\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 2, Iteration 16400\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 2, Iteration 16600\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 2, Iteration 16800\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 2, Iteration 17000\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 17200\t train_loss: 0.24 took: 1.76s\n",
            "Epoch 2, Iteration 17400\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 2, Iteration 17600\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 2, Iteration 17800\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 2, Iteration 18000\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 2, Iteration 18200\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 2, Iteration 18400\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 2, Iteration 18600\t train_loss: 0.27 took: 1.66s\n",
            "Epoch 2, Iteration 18800\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 2, Iteration 19000\t train_loss: 0.29 took: 1.67s\n",
            "Epoch 2, Iteration 19200\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 2, Iteration 19400\t train_loss: 0.27 took: 1.64s\n",
            "Epoch 2, Iteration 19600\t train_loss: 0.27 took: 1.75s\n",
            "Epoch 2, Iteration 19800\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 2, Iteration 20000\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 2, Iteration 20200\t train_loss: 0.25 took: 1.70s\n",
            "Epoch 2, Iteration 20400\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 2, Iteration 20600\t train_loss: 0.25 took: 1.72s\n",
            "Epoch 2, Iteration 20800\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 2, Iteration 21000\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 2, Iteration 21200\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 2, Iteration 21400\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 2, Iteration 21600\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 2, Iteration 21800\t train_loss: 0.28 took: 1.64s\n",
            "Epoch 2, Iteration 22000\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 2, Iteration 22200\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 2, Iteration 22400\t train_loss: 0.25 took: 1.70s\n",
            "Epoch 2, Iteration 22600\t train_loss: 0.28 took: 1.73s\n",
            "Epoch 2, Iteration 22800\t train_loss: 0.27 took: 1.70s\n",
            "Epoch 2, Iteration 23000\t train_loss: 0.24 took: 1.63s\n",
            "Validation loss = 0.26\n",
            "Epoch 3, Iteration 200\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 3, Iteration 400\t train_loss: 0.26 took: 1.78s\n",
            "Epoch 3, Iteration 600\t train_loss: 0.25 took: 1.72s\n",
            "Epoch 3, Iteration 800\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 3, Iteration 1000\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 3, Iteration 1200\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 1400\t train_loss: 0.23 took: 1.65s\n",
            "Epoch 3, Iteration 1600\t train_loss: 0.26 took: 1.60s\n",
            "Epoch 3, Iteration 1800\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 3, Iteration 2000\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 3, Iteration 2200\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 3, Iteration 2400\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 3, Iteration 2600\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 3, Iteration 2800\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 3, Iteration 3000\t train_loss: 0.26 took: 1.65s\n",
            "Epoch 3, Iteration 3200\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 3400\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 3, Iteration 3600\t train_loss: 0.26 took: 1.69s\n",
            "Epoch 3, Iteration 3800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 4000\t train_loss: 0.24 took: 1.77s\n",
            "Epoch 3, Iteration 4200\t train_loss: 0.23 took: 1.71s\n",
            "Epoch 3, Iteration 4400\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 3, Iteration 4600\t train_loss: 0.25 took: 1.61s\n",
            "Epoch 3, Iteration 4800\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 3, Iteration 5000\t train_loss: 0.26 took: 1.71s\n",
            "Epoch 3, Iteration 5200\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 3, Iteration 5400\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 3, Iteration 5600\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 3, Iteration 5800\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 3, Iteration 6000\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 3, Iteration 6200\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 3, Iteration 6400\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 3, Iteration 6600\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 3, Iteration 6800\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 3, Iteration 7000\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 3, Iteration 7200\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 7400\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 3, Iteration 7600\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 3, Iteration 7800\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 3, Iteration 8000\t train_loss: 0.26 took: 1.76s\n",
            "Epoch 3, Iteration 8200\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 8400\t train_loss: 0.27 took: 1.79s\n",
            "Epoch 3, Iteration 8600\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 3, Iteration 8800\t train_loss: 0.27 took: 1.65s\n",
            "Epoch 3, Iteration 9000\t train_loss: 0.27 took: 1.78s\n",
            "Epoch 3, Iteration 9200\t train_loss: 0.27 took: 1.63s\n",
            "Epoch 3, Iteration 9400\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 3, Iteration 9600\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 3, Iteration 9800\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 3, Iteration 10000\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 3, Iteration 10200\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 3, Iteration 10400\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 3, Iteration 10600\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 3, Iteration 10800\t train_loss: 0.24 took: 1.74s\n",
            "Epoch 3, Iteration 11000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 11200\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 3, Iteration 11400\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 3, Iteration 11600\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 3, Iteration 11800\t train_loss: 0.26 took: 1.75s\n",
            "Epoch 3, Iteration 12000\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 3, Iteration 12200\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 3, Iteration 12400\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 3, Iteration 12600\t train_loss: 0.25 took: 1.60s\n",
            "Epoch 3, Iteration 12800\t train_loss: 0.26 took: 1.65s\n",
            "Epoch 3, Iteration 13000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 13200\t train_loss: 0.25 took: 1.78s\n",
            "Epoch 3, Iteration 13400\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 3, Iteration 13600\t train_loss: 0.26 took: 1.69s\n",
            "Epoch 3, Iteration 13800\t train_loss: 0.27 took: 1.66s\n",
            "Epoch 3, Iteration 14000\t train_loss: 0.27 took: 1.67s\n",
            "Epoch 3, Iteration 14200\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 3, Iteration 14400\t train_loss: 0.25 took: 1.70s\n",
            "Epoch 3, Iteration 14600\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 3, Iteration 14800\t train_loss: 0.23 took: 1.63s\n",
            "Epoch 3, Iteration 15000\t train_loss: 0.26 took: 1.73s\n",
            "Epoch 3, Iteration 15200\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 3, Iteration 15400\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 3, Iteration 15600\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 3, Iteration 15800\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 3, Iteration 16000\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 3, Iteration 16200\t train_loss: 0.24 took: 1.82s\n",
            "Epoch 3, Iteration 16400\t train_loss: 0.26 took: 1.71s\n",
            "Epoch 3, Iteration 16600\t train_loss: 0.26 took: 1.65s\n",
            "Epoch 3, Iteration 16800\t train_loss: 0.24 took: 1.71s\n",
            "Epoch 3, Iteration 17000\t train_loss: 0.26 took: 1.79s\n",
            "Epoch 3, Iteration 17200\t train_loss: 0.26 took: 1.65s\n",
            "Epoch 3, Iteration 17400\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 17600\t train_loss: 0.23 took: 1.64s\n",
            "Epoch 3, Iteration 17800\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 3, Iteration 18000\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 3, Iteration 18200\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 3, Iteration 18400\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 3, Iteration 18600\t train_loss: 0.26 took: 1.69s\n",
            "Epoch 3, Iteration 18800\t train_loss: 0.26 took: 1.71s\n",
            "Epoch 3, Iteration 19000\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 3, Iteration 19200\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 3, Iteration 19400\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 3, Iteration 19600\t train_loss: 0.23 took: 1.66s\n",
            "Epoch 3, Iteration 19800\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 3, Iteration 20000\t train_loss: 0.26 took: 1.69s\n",
            "Epoch 3, Iteration 20200\t train_loss: 0.27 took: 1.71s\n",
            "Epoch 3, Iteration 20400\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 3, Iteration 20600\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 3, Iteration 20800\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 3, Iteration 21000\t train_loss: 0.26 took: 1.74s\n",
            "Epoch 3, Iteration 21200\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 3, Iteration 21400\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 3, Iteration 21600\t train_loss: 0.24 took: 1.75s\n",
            "Epoch 3, Iteration 21800\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 3, Iteration 22000\t train_loss: 0.24 took: 1.61s\n",
            "Epoch 3, Iteration 22200\t train_loss: 0.25 took: 1.61s\n",
            "Epoch 3, Iteration 22400\t train_loss: 0.23 took: 1.63s\n",
            "Epoch 3, Iteration 22600\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 3, Iteration 22800\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 3, Iteration 23000\t train_loss: 0.25 took: 1.73s\n",
            "Validation loss = 0.27\n",
            "Epoch 4, Iteration 200\t train_loss: 0.25 took: 1.78s\n",
            "Epoch 4, Iteration 400\t train_loss: 0.23 took: 1.83s\n",
            "Epoch 4, Iteration 600\t train_loss: 0.25 took: 1.70s\n",
            "Epoch 4, Iteration 800\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 4, Iteration 1000\t train_loss: 0.24 took: 1.61s\n",
            "Epoch 4, Iteration 1200\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 1400\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 4, Iteration 1600\t train_loss: 0.25 took: 1.79s\n",
            "Epoch 4, Iteration 1800\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 4, Iteration 2000\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 4, Iteration 2200\t train_loss: 0.25 took: 1.58s\n",
            "Epoch 4, Iteration 2400\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 2600\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 4, Iteration 2800\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 4, Iteration 3000\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 4, Iteration 3200\t train_loss: 0.25 took: 1.74s\n",
            "Epoch 4, Iteration 3400\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 4, Iteration 3600\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 4, Iteration 3800\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 4, Iteration 4000\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 4, Iteration 4200\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 4400\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 4, Iteration 4600\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 4, Iteration 4800\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 4, Iteration 5000\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 5200\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 4, Iteration 5400\t train_loss: 0.24 took: 1.74s\n",
            "Epoch 4, Iteration 5600\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 4, Iteration 5800\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 4, Iteration 6000\t train_loss: 0.22 took: 1.67s\n",
            "Epoch 4, Iteration 6200\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 4, Iteration 6400\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 4, Iteration 6600\t train_loss: 0.24 took: 1.71s\n",
            "Epoch 4, Iteration 6800\t train_loss: 0.24 took: 1.74s\n",
            "Epoch 4, Iteration 7000\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 4, Iteration 7200\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 7400\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 4, Iteration 7600\t train_loss: 0.24 took: 1.61s\n",
            "Epoch 4, Iteration 7800\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 4, Iteration 8000\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 4, Iteration 8200\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 4, Iteration 8400\t train_loss: 0.24 took: 1.77s\n",
            "Epoch 4, Iteration 8600\t train_loss: 0.25 took: 1.70s\n",
            "Epoch 4, Iteration 8800\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 4, Iteration 9000\t train_loss: 0.25 took: 1.82s\n",
            "Epoch 4, Iteration 9200\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 9400\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 9600\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 4, Iteration 9800\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 4, Iteration 10000\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 4, Iteration 10200\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 4, Iteration 10400\t train_loss: 0.22 took: 1.77s\n",
            "Epoch 4, Iteration 10600\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 4, Iteration 10800\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 4, Iteration 11000\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 11200\t train_loss: 0.23 took: 1.65s\n",
            "Epoch 4, Iteration 11400\t train_loss: 0.23 took: 1.67s\n",
            "Epoch 4, Iteration 11600\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 4, Iteration 11800\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 4, Iteration 12000\t train_loss: 0.24 took: 1.71s\n",
            "Epoch 4, Iteration 12200\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 4, Iteration 12400\t train_loss: 0.23 took: 1.71s\n",
            "Epoch 4, Iteration 12600\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 4, Iteration 12800\t train_loss: 0.23 took: 1.67s\n",
            "Epoch 4, Iteration 13000\t train_loss: 0.24 took: 1.73s\n",
            "Epoch 4, Iteration 13200\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 4, Iteration 13400\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 4, Iteration 13600\t train_loss: 0.25 took: 1.61s\n",
            "Epoch 4, Iteration 13800\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 4, Iteration 14000\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 4, Iteration 14200\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 14400\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 14600\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 4, Iteration 14800\t train_loss: 0.24 took: 1.70s\n",
            "Epoch 4, Iteration 15000\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 4, Iteration 15200\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 15400\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 4, Iteration 15600\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 4, Iteration 15800\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 4, Iteration 16000\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 4, Iteration 16200\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 16400\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 4, Iteration 16600\t train_loss: 0.25 took: 1.77s\n",
            "Epoch 4, Iteration 16800\t train_loss: 0.24 took: 1.71s\n",
            "Epoch 4, Iteration 17000\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 4, Iteration 17200\t train_loss: 0.27 took: 1.69s\n",
            "Epoch 4, Iteration 17400\t train_loss: 0.23 took: 1.65s\n",
            "Epoch 4, Iteration 17600\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 4, Iteration 17800\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 4, Iteration 18000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 18200\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 4, Iteration 18400\t train_loss: 0.23 took: 1.66s\n",
            "Epoch 4, Iteration 18600\t train_loss: 0.25 took: 1.61s\n",
            "Epoch 4, Iteration 18800\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 4, Iteration 19000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 19200\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 4, Iteration 19400\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 4, Iteration 19600\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 4, Iteration 19800\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 4, Iteration 20000\t train_loss: 0.23 took: 1.62s\n",
            "Epoch 4, Iteration 20200\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 4, Iteration 20400\t train_loss: 0.24 took: 1.76s\n",
            "Epoch 4, Iteration 20600\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 20800\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 21000\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 21200\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 4, Iteration 21400\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 4, Iteration 21600\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 21800\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 4, Iteration 22000\t train_loss: 0.23 took: 1.63s\n",
            "Epoch 4, Iteration 22200\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 22400\t train_loss: 0.25 took: 1.62s\n",
            "Epoch 4, Iteration 22600\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 4, Iteration 22800\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 4, Iteration 23000\t train_loss: 0.26 took: 1.65s\n",
            "Validation loss = 0.25\n",
            "Epoch 5, Iteration 200\t train_loss: 0.22 took: 1.85s\n",
            "Epoch 5, Iteration 400\t train_loss: 0.23 took: 1.66s\n",
            "Epoch 5, Iteration 600\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 5, Iteration 800\t train_loss: 0.23 took: 1.69s\n",
            "Epoch 5, Iteration 1000\t train_loss: 0.22 took: 1.67s\n",
            "Epoch 5, Iteration 1200\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 5, Iteration 1400\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 5, Iteration 1600\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 5, Iteration 1800\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 5, Iteration 2000\t train_loss: 0.23 took: 1.63s\n",
            "Epoch 5, Iteration 2200\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 5, Iteration 2400\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 5, Iteration 2600\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 5, Iteration 2800\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 5, Iteration 3000\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 5, Iteration 3200\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 5, Iteration 3400\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 5, Iteration 3600\t train_loss: 0.23 took: 1.66s\n",
            "Epoch 5, Iteration 3800\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 5, Iteration 4000\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 5, Iteration 4200\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 5, Iteration 4400\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 5, Iteration 4600\t train_loss: 0.24 took: 1.70s\n",
            "Epoch 5, Iteration 4800\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 5, Iteration 5000\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 5, Iteration 5200\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 5, Iteration 5400\t train_loss: 0.26 took: 1.68s\n",
            "Epoch 5, Iteration 5600\t train_loss: 0.27 took: 1.67s\n",
            "Epoch 5, Iteration 5800\t train_loss: 0.24 took: 1.63s\n",
            "Epoch 5, Iteration 6000\t train_loss: 0.24 took: 1.73s\n",
            "Epoch 5, Iteration 6200\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 5, Iteration 6400\t train_loss: 0.25 took: 1.73s\n",
            "Epoch 5, Iteration 6600\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 5, Iteration 6800\t train_loss: 0.23 took: 1.78s\n",
            "Epoch 5, Iteration 7000\t train_loss: 0.27 took: 1.74s\n",
            "Epoch 5, Iteration 7200\t train_loss: 0.22 took: 1.71s\n",
            "Epoch 5, Iteration 7400\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 5, Iteration 7600\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 5, Iteration 7800\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 5, Iteration 8000\t train_loss: 0.23 took: 1.75s\n",
            "Epoch 5, Iteration 8200\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 5, Iteration 8400\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 5, Iteration 8600\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 5, Iteration 8800\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 5, Iteration 9000\t train_loss: 0.23 took: 1.68s\n",
            "Epoch 5, Iteration 9200\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 5, Iteration 9400\t train_loss: 0.22 took: 1.68s\n",
            "Epoch 5, Iteration 9600\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 5, Iteration 9800\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 5, Iteration 10000\t train_loss: 0.24 took: 1.74s\n",
            "Epoch 5, Iteration 10200\t train_loss: 0.23 took: 1.68s\n",
            "Epoch 5, Iteration 10400\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 5, Iteration 10600\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 5, Iteration 10800\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 5, Iteration 11000\t train_loss: 0.23 took: 1.64s\n",
            "Epoch 5, Iteration 11200\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 5, Iteration 11400\t train_loss: 0.23 took: 1.62s\n",
            "Epoch 5, Iteration 11600\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 5, Iteration 11800\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 5, Iteration 12000\t train_loss: 0.23 took: 1.65s\n",
            "Epoch 5, Iteration 12200\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 5, Iteration 12400\t train_loss: 0.25 took: 1.69s\n",
            "Epoch 5, Iteration 12600\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 5, Iteration 12800\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 5, Iteration 13000\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 5, Iteration 13200\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 5, Iteration 13400\t train_loss: 0.23 took: 1.74s\n",
            "Epoch 5, Iteration 13600\t train_loss: 0.24 took: 1.77s\n",
            "Epoch 5, Iteration 13800\t train_loss: 0.25 took: 1.67s\n",
            "Epoch 5, Iteration 14000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 5, Iteration 14200\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 5, Iteration 14400\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 5, Iteration 14600\t train_loss: 0.23 took: 1.66s\n",
            "Epoch 5, Iteration 14800\t train_loss: 0.24 took: 1.65s\n",
            "Epoch 5, Iteration 15000\t train_loss: 0.25 took: 1.65s\n",
            "Epoch 5, Iteration 15200\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 5, Iteration 15400\t train_loss: 0.24 took: 1.62s\n",
            "Epoch 5, Iteration 15600\t train_loss: 0.27 took: 1.64s\n",
            "Epoch 5, Iteration 15800\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 5, Iteration 16000\t train_loss: 0.26 took: 1.72s\n",
            "Epoch 5, Iteration 16200\t train_loss: 0.26 took: 1.64s\n",
            "Epoch 5, Iteration 16400\t train_loss: 0.23 took: 1.68s\n",
            "Epoch 5, Iteration 16600\t train_loss: 0.25 took: 1.82s\n",
            "Epoch 5, Iteration 16800\t train_loss: 0.24 took: 1.71s\n",
            "Epoch 5, Iteration 17000\t train_loss: 0.26 took: 1.70s\n",
            "Epoch 5, Iteration 17200\t train_loss: 0.28 took: 1.65s\n",
            "Epoch 5, Iteration 17400\t train_loss: 0.24 took: 1.69s\n",
            "Epoch 5, Iteration 17600\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 5, Iteration 17800\t train_loss: 0.25 took: 1.63s\n",
            "Epoch 5, Iteration 18000\t train_loss: 0.24 took: 1.67s\n",
            "Epoch 5, Iteration 18200\t train_loss: 0.23 took: 1.64s\n",
            "Epoch 5, Iteration 18400\t train_loss: 0.23 took: 1.63s\n",
            "Epoch 5, Iteration 18600\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 5, Iteration 18800\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 5, Iteration 19000\t train_loss: 0.23 took: 1.80s\n",
            "Epoch 5, Iteration 19200\t train_loss: 0.25 took: 1.76s\n",
            "Epoch 5, Iteration 19400\t train_loss: 0.26 took: 1.67s\n",
            "Epoch 5, Iteration 19600\t train_loss: 0.24 took: 1.66s\n",
            "Epoch 5, Iteration 19800\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 5, Iteration 20000\t train_loss: 0.26 took: 1.62s\n",
            "Epoch 5, Iteration 20200\t train_loss: 0.26 took: 1.66s\n",
            "Epoch 5, Iteration 20400\t train_loss: 0.24 took: 1.72s\n",
            "Epoch 5, Iteration 20600\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 5, Iteration 20800\t train_loss: 0.24 took: 1.76s\n",
            "Epoch 5, Iteration 21000\t train_loss: 0.26 took: 1.63s\n",
            "Epoch 5, Iteration 21200\t train_loss: 0.25 took: 1.71s\n",
            "Epoch 5, Iteration 21400\t train_loss: 0.24 took: 1.77s\n",
            "Epoch 5, Iteration 21600\t train_loss: 0.25 took: 1.64s\n",
            "Epoch 5, Iteration 21800\t train_loss: 0.25 took: 1.68s\n",
            "Epoch 5, Iteration 22000\t train_loss: 0.26 took: 1.65s\n",
            "Epoch 5, Iteration 22200\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 5, Iteration 22400\t train_loss: 0.25 took: 1.66s\n",
            "Epoch 5, Iteration 22600\t train_loss: 0.24 took: 1.68s\n",
            "Epoch 5, Iteration 22800\t train_loss: 0.24 took: 1.64s\n",
            "Epoch 5, Iteration 23000\t train_loss: 0.24 took: 1.69s\n",
            "Validation loss = 0.25\n",
            "Training finished, took 989.67s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNSsFzVji6W",
        "colab_type": "code",
        "outputId": "21b4ed41-c604-4494-e361-b561b24e8053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlim(0,12000)\n",
        "plt.ylim(0,2)\n",
        "plt.show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ13LfcF3Ji2",
        "colab_type": "code",
        "outputId": "0f721d4d-302e-4a43-d069-acc50eef61d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.922698531833669\n",
            "Test accuracy is 0.9179565058923581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMZxrWj47rA",
        "colab_type": "code",
        "outputId": "3ca80f71-49a7-4483-d84d-089bccb66988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    confidence = class_scores.cpu().detach().numpy()\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "    print(prediction)\n",
        "    print(label)\n",
        "    print(max(confidence[0])/sum(confidence[0]))\n",
        "    print(confidence)\n",
        "\n",
        "examine_label(11125)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1P8BaW7nPheRPjgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "4\n",
            "0.0018392931847716512\n",
            "[[-8.180107   -3.4528627  -4.5043907  -5.931608   -0.05086613 -5.535423  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSeuwv8R5NGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}